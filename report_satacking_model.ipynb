{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7467d-e5a8-4665-bfdb-2aa5a9af3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import math\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import set_config\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from itertools import product\n",
    "from joblib import dump\n",
    "from scipy import stats\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "from yellowbrick.model_selection import learning_curve\n",
    "from yellowbrick.model_selection import feature_importances\n",
    "from yellowbrick.features import rank1d\n",
    "from yellowbrick.features import rank2d\n",
    "from yellowbrick.contrib.missing import MissingValuesBar\n",
    "from yellowbrick.contrib.missing import MissingValuesDispersion\n",
    "from yellowbrick.target.feature_correlation import feature_correlation\n",
    "from yellowbrick.regressor import prediction_error\n",
    "from yellowbrick.regressor import residuals_plot\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sys\n",
    "from scipy import stats\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.base import BaseEstimator, is_classifier, is_regressor, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, recall_score \n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR  # <-- 確保引入 SVR\n",
    "from sklearn.linear_model import BayesianRidge, Lasso\n",
    "from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d7842-2bff-45ce-9bd2-341f8c164b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataframe_structure(df):\n",
    "    \"\"\"\n",
    "    Plot dataframe structure: It shows the different data types in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    df.dtypes.value_counts().plot.pie(ylabel='')\n",
    "    plt.title('Data types')\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical(df):\n",
    "    \"\"\"\n",
    "    Plot the number of different values for each categorical feature in the dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Plotting\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    df.nunique().plot.bar()\n",
    "    plt.title('Number of different values')\n",
    "    plt.show()\n",
    "    \n",
    "def duplicates(df):\n",
    "    \"\"\"\n",
    "    Remove the duplicate rows from dataframe.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe without duplicate rows \n",
    "    \"\"\"    \n",
    "    duplicate_rows_df = df[df.duplicated()]\n",
    "    if duplicate_rows_df.shape[0] > 0:\n",
    "       print('Number of rows before removing:', df.count()[0])\n",
    "       print('Number of duplicate rows:', duplicate_rows_df.shape[0])\n",
    "       df = df.drop_duplicates()\n",
    "       print('Number of rows after removing:', df.count()[0])\n",
    "    else:\n",
    "       print('No duplicate rows.')\n",
    "    return df\n",
    "\n",
    "def drop_na(df, threshold_NaN):\n",
    "    \"\"\"\n",
    "    Remove the columns from dataframe containing NaN depending on threshold_NaN.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    threshold_NaN: in [0, 1], from GUI\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe \n",
    "    drop_cols: list of dropped columns\n",
    "    \"\"\"    \n",
    "    isna_stat = (df.isna().sum()/df.shape[0]).sort_values(ascending=True)\n",
    "    drop_cols = []\n",
    "    if isna_stat.max() > 0.0:\n",
    "       drop_cols = np.array(isna_stat[isna_stat > threshold_NaN].index)\n",
    "       print('Drop columns containing more than', threshold_NaN*100,'% of NaN:', drop_cols)\n",
    "       df = df.drop(drop_cols, axis=1)\n",
    "    else:\n",
    "       print('No need to drop columns.')\n",
    "    return df, drop_cols\n",
    "\n",
    "def encoding(df, threshold_cat, target_col):\n",
    "    \"\"\"\n",
    "    Encode the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    threshold_cat: integer, if the number of different values of a given column is less than this limit, \n",
    "    this column is considered as categorical. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe \n",
    "    encoded_cols: Pandas dataframe of columns with their encoding and range\n",
    "    \"\"\"      \n",
    "    encoded_cols = []\n",
    "    drop_cols = np.array([])  # Initialize drop_cols as an empty array\n",
    "    \n",
    "    for c in df.columns:\n",
    "        if df[c].dtypes == 'object' or df[c].dtypes.name == 'category': \n",
    "            encoded_cols.append([c, 'cat', df[c].dropna().unique().tolist()])\n",
    "            print('Encoding object column:', c)\n",
    "            df[c] = df[c].factorize()[0].astype(int)\n",
    "        elif is_numeric_dtype(df[c]): \n",
    "            if df[c].unique().shape[0] > threshold_cat: \n",
    "                encoded_cols.append([c, 'num', [df[c].min(), df[c].max()]])\n",
    "                print('Encoding numeric column:', c)\n",
    "                df[c] = (df[c] - df[c].mean()) / df[c].std()\n",
    "            else:\n",
    "                print('Column ', c, ' is categorical.')\n",
    "                encoded_cols.append([c, 'cat', df[c].dropna().unique().tolist()])\n",
    "        else: \n",
    "            print('Unknown type ', df[c].dtypes, ' for column:', c) \n",
    "            df = df.drop(c, axis=1)\n",
    "            drop_cols = np.unique(np.concatenate((drop_cols, [c])))  # Add column to drop_cols\n",
    "\n",
    "    encoded_cols = pd.DataFrame(encoded_cols, columns=['column_name', 'column_type', 'column_range'])\n",
    "    encoded_cols = encoded_cols.loc[encoded_cols['column_name'] != target_col]\n",
    "    encoded_cols.to_csv('schema.csv', index=False)\n",
    "    \n",
    "    return df, encoded_cols\n",
    "\n",
    "def imputation(df):\n",
    "    \"\"\"\n",
    "    Impute NaN in the dataframe using IterativeImputer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe \n",
    "    \"\"\"        \n",
    "    isna_stat = (df.isna().sum()/df.shape[0]).sort_values(ascending=True) \n",
    "    if isna_stat.max() > 0.0: \n",
    "       print('Imputing NaN using IterativeImputer') \n",
    "       df = pd.DataFrame(IterativeImputer(random_state=0).fit_transform(df), columns = df.columns)  \n",
    "    else: \n",
    "       print('No need to impute data.')\n",
    "    return df\n",
    "\n",
    "def outliers(df, threshold_Z):\n",
    "    \"\"\"\n",
    "    Remove the outliers from dataframe according to Z_score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    threshold_Z: number from GUI. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe. \n",
    "    \"\"\"  \n",
    "    Z_score = np.abs(stats.zscore(df)) \n",
    "    df_o_Z = df[(Z_score < threshold_Z).all(axis=1)]\n",
    "    if df_o_Z.shape[0] != 0:\n",
    "       print('Using Z_score, ', str(df.shape[0] - df_o_Z.shape[0]) ,' rows will be suppressed.') \n",
    "       df = df_o_Z\n",
    "    else:\n",
    "       print('Possible problem with outliers treatment, check threshold_Z') \n",
    "    return df\n",
    "\n",
    "def correlated_columns(df, threshold_corr, target_col):\n",
    "    \"\"\"\n",
    "    Display correlation matrix of features, and returns the list of the too correlated features\n",
    "    according to threshold_corr.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    threshold_corr: number from GUI\n",
    "    target: target column\n",
    "    Returns\n",
    "    -------\n",
    "    correlated_features: list of the features having a correlation greater than threshold_corr. \n",
    "    \"\"\"  \n",
    "    df = df.drop(target_col, axis=1)\n",
    "    corr_matrix = df.corr() \n",
    "    correlated_features=[]\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold_corr: # we are interested in absolute coeff value\n",
    "               colname = corr_matrix.columns[i]  # getting the name of column\n",
    "               correlated_features.append(colname)\n",
    "    correlated_features = list(dict.fromkeys(correlated_features))\n",
    "    return correlated_features\n",
    "\n",
    "def plot_sns_corr_class(df, target_col):\n",
    "    \"\"\"\n",
    "    Plot correlation information for classification problem (if Seaborn option is checked).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    target_col: name of the target column. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Plotting. \n",
    "    \"\"\"     \n",
    "    g = sns.PairGrid(df, hue=target_col) \n",
    "    g.map_upper(sns.scatterplot) \n",
    "    g.map_lower(sns.kdeplot) \n",
    "    g.map_diag(sns.kdeplot, lw=3, legend=False) \n",
    "    g.add_legend() \n",
    "    g.fig.suptitle('Pairwise data relationships', y=1.01) \n",
    "    plt.show()\n",
    "    \n",
    "def plot_sns_corr_regre(df, target_col):\n",
    "    \"\"\"\n",
    "    Plot correlation information for regression problem (if Seaborn option is checked).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    target_col: name of the target column. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Plotting. \n",
    "    \"\"\"      \n",
    "    g = sns.PairGrid(df)\n",
    "    g.map_upper(sns.scatterplot)\n",
    "    g.map_lower(sns.kdeplot)\n",
    "    g.map_diag(sns.kdeplot, lw=3, legend=False)\n",
    "    g.fig.suptitle('Pairwise data relationships', y=1.01)\n",
    "    plt.show()\n",
    "    \n",
    "class Decorrelator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Decorrelator is a class used to eliminate too correlated columns depending on a threshold during preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold_corr\n",
    "    \"\"\"  \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        self.correlated_columns = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        correlated_features = set()  \n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "           X = pd.DataFrame(X)\n",
    "        corr_matrix = X.corr()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.threshold: # we are interested in absolute coeff value\n",
    "                    colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                    correlated_features.add(colname)\n",
    "        self.correlated_features = correlated_features\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **kwargs):\n",
    "        return (pd.DataFrame(X)).drop(labels=self.correlated_features, axis=1)\n",
    "    \n",
    "class ColumnsDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    ColumnsDropper is a class used to drop columns from a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cols : list of columns dropped by the transformer\n",
    "    \"\"\"  \n",
    "    def __init__(self, cols):\n",
    "        if not isinstance(cols, list):\n",
    "            self.cols = [cols]\n",
    "        else:\n",
    "            self.cols = cols\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        # there is nothing to fit\n",
    "        return self\n",
    "\n",
    "    def transform(self, X:pd.DataFrame):\n",
    "        X = X.copy()\n",
    "        return X[self.cols]    \n",
    "    \n",
    "def model_filtering(level_0, model_imp, nb_model, score_stack, threshold_score):\n",
    "    \"\"\"\n",
    "    Suppress estimators from level 0 having a test score smaller than threshold_score (from score_stack), then \n",
    "    keep nb_model best estimators (according to model_imp).\n",
    "    Parameters\n",
    "    ----------\n",
    "    level_0: list of estimators of level 0\n",
    "    model_imp: sorted array of model importance\n",
    "    nb_model : number of model to keep\n",
    "    score_stack: accuracy of estimators on train and test sets in a tabular\n",
    "    threshold_score : minimal score\n",
    "    encoded_cols = pd.DataFrame(np.array(encoded_\n",
    "    Returns\n",
    "    -------\n",
    "    list of filtered estimators of level 0\n",
    "    \"\"\"\n",
    "    # it is not possible to keep more models than we initially have\n",
    "    if nb_model > len(level_0):\n",
    "       nb_model = len(level_0)\n",
    "    \n",
    "    # keep model names and test scores\n",
    "    score_stack = np.delete(np.delete(score_stack, 1, axis =1), -1, axis = 0)\n",
    "    # keep models having test score greater than threshold_score \n",
    "    score_stack = score_stack[score_stack[:,1] > threshold_score]\n",
    "    \n",
    "    # it is not possible to keep more models than we have filtered    \n",
    "    if nb_model > len(score_stack):\n",
    "       nb_model = len(score_stack)\n",
    "    \n",
    "    # keep models (in importance array) having test score greater than threshold_score\n",
    "    model_imp = model_imp[np.in1d(model_imp[:, 0], score_stack)]\n",
    "    model_imp_f = model_imp[np.argpartition(model_imp[:,1], -nb_model)[-nb_model:]].T[0]\n",
    "    \n",
    "    return list(filter(lambda x: x[0] in model_imp_f, level_0))\n",
    "\n",
    "def feature_filtering(feature_importance, nb_feature):\n",
    "    \"\"\"\n",
    "    Separate features in two lists, the first one contains the nb_feature most important features, \n",
    "    the second one contains the complement\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_importance: array of features with their importance\n",
    "    nb_feature: number of features we want to keep\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_feature: list of nb_feature most important features\n",
    "    worst_feature: list of the worst important features\n",
    "    \"\"\"\n",
    "    # check nb_feature\n",
    "    if nb_feature > feature_importance.shape[0]:\n",
    "       nb_feature = feature_importance.shape[0] \n",
    "    \n",
    "    best_feature = feature_importance[np.argpartition(feature_importance[:,1], -nb_feature)[-nb_feature:]].T[0]\n",
    "    worst_feature = list(set(feature_importance.T[0]) - set(best_feature))\n",
    "\n",
    "    return best_feature, worst_feature\n",
    "\n",
    "def split(X, y, random_state, test_size=0.33, threshold_entropy=0.7, undersampling=False, undersampler=None):\n",
    "    \"\"\"\n",
    "    Split dataframe into train and test sets.\n",
    "    If the Shannon entropy of the target dataset is less than 0.7, RepeatedStratifiedKFold is used\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: feature dataframe\n",
    "    y: target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \"\"\"\n",
    "    s_e = shannon_entropy(y)\n",
    "    if s_e < threshold_entropy:\n",
    "       if undersampling: \n",
    "          if undersampler == 'Random': \n",
    "             from imblearn.under_sampling import RandomUnderSampler\n",
    "             us = RandomUnderSampler()\n",
    "          elif undersampler == 'Centroids': \n",
    "             from imblearn.under_sampling import ClusterCentroids\n",
    "             us = ClusterCentroids()\n",
    "          elif undersampler == 'AllKNN': \n",
    "             from imblearn.under_sampling import AllKNN\n",
    "             us = AllKNN()\n",
    "          elif undersampler == 'TomekLinks': \n",
    "             from imblearn.under_sampling import TomekLinks\n",
    "             us = TomekLinks()\n",
    "          else:\n",
    "             print(\"Unknown undersampler\")       \n",
    "          X, y = us.fit_resample(X, y)\n",
    "          print(\"Shannon Entropy = {:.4}, split using undersampler {} and RepeatedStratifiedKFold\".format(s_e, undersampler)) \n",
    "       else: \n",
    "          print(\"Shannon Entropy = {:.4}, split using RepeatedStratifiedKFold\".format(s_e)) \n",
    "       skfold = RepeatedStratifiedKFold(n_splits=5, random_state = random_state)\n",
    "       # enumerate the splits and summarize the distributions\n",
    "       for ind_train, ind_test in skfold.split(X, y):\n",
    "           X_train, X_test = X.iloc[ind_train], X.iloc[ind_test]\n",
    "           y_train, y_test = y.iloc[ind_train], y.iloc[ind_test] \n",
    "    else:    \n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=None,\\\n",
    "                                                           shuffle=True, random_state = random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def downcast_dtypes(df):\n",
    "    \"\"\"\n",
    "    Compress dataframe\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: Pandas dataframe\n",
    "    \"\"\"      \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage of dataframe is {:.2f}' \n",
    "                     'MB').format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(('Memory usage after optimization is: {:.2f}' \n",
    "                              'MB').format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def shannon_entropy(y):\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy of a dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: univariate Pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    shannon entropy: float\n",
    "    \"\"\"     \n",
    "    from collections import Counter\n",
    "    from numpy import log\n",
    "    \n",
    "    n = len(y)\n",
    "    classes = [(clas,float(count)) for clas,count in Counter(y).items()]\n",
    "    k = len(classes)\n",
    "    \n",
    "    H = -sum([ (count/n) * log((count/n)) for clas,count in classes]) #shannon entropy\n",
    "    return H/log(k)\n",
    "\n",
    "def score_stacking_c(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute the score of the stacked classification estimator and of each level_0 estimator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: accuracy of estimators on train and test sets\n",
    "    res_stack: accuracy of estimators on train and test sets in a tabular\n",
    "    \"\"\"       \n",
    "    nb_estimators = len(model.estimators_)\n",
    "    res_stack = np.empty((nb_estimators + 1, 3), dtype='object')\n",
    "    m_t_x_train = model.transform(X_train)\n",
    "    for j in range(nb_estimators):\n",
    "        res_stack [j, 0] = [*model.named_estimators_.keys()][j]\n",
    "        if m_t_x_train.shape[1] == nb_estimators: \n",
    "           res_stack [j, 1] = accuracy_score(np.rint(m_t_x_train).T[j], y_train)\n",
    "           res_stack [j, 2] = accuracy_score(np.rint(model.transform(X_test)).T[j], y_test)\n",
    "        else: \n",
    "           res_stack [j, 1] = accuracy_score(m_t_x_train.reshape((X_train.shape[0],\\\n",
    "                                                                  nb_estimators,\\\n",
    "                                                                  y_train.unique().shape[0])).argmax(axis=2).T[j],\\\n",
    "                                             y_train)\n",
    "           res_stack [j, 2] = accuracy_score(model.transform(X_test).reshape((X_test.shape[0],\\\n",
    "                                                                              nb_estimators,\\\n",
    "                                                                              y_test.unique().shape[0])).argmax(axis=2).T[j],\\\n",
    "                                             y_test)\n",
    "    res_stack [len(model.estimators_) , 0] = 'Stack'\n",
    "    res_stack [len(model.estimators_) , 1] = accuracy_score(model.predict(X_train), y_train)\n",
    "    res_stack [len(model.estimators_) , 2] = accuracy_score(model.predict(X_test), y_test)  \n",
    "    models = res_stack.T[0]\n",
    "    score_train = res_stack.T[1]\n",
    "    score_test = res_stack.T[2]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.scatter(models, score_train, label='Train')\n",
    "    plt.scatter(models, score_test, label='Test')\n",
    "    plt.title('Model scores: accuracy')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return res_stack\n",
    "\n",
    "def score_stacking_r(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute the score of the stacked regression estimator and of each level_0 estimator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: accuracy of estimators on train and test sets\n",
    "    res_stack: accuracy of estimators on train and test sets in a tabular\n",
    "    \"\"\"        \n",
    "    nb_estimators = len(model.estimators_)\n",
    "    res_stack = np.empty((nb_estimators + 1, 3), dtype='object')\n",
    "    m_t_x_train = model.transform(X_train)\n",
    "    for j in range(nb_estimators):\n",
    "        res_stack [j, 0] = [*model.named_estimators_.keys()][j]\n",
    "        res_stack [j, 1] = r2_score(np.rint(m_t_x_train).T[j], y_train)\n",
    "        res_stack [j, 2] = r2_score(np.rint(model.transform(X_test)).T[j], y_test)\n",
    "    res_stack [len(model.estimators_) , 0] = 'Stack'\n",
    "    res_stack [len(model.estimators_) , 1] = r2_score(model.predict(X_train), y_train)\n",
    "    res_stack [len(model.estimators_) , 2] = r2_score(model.predict(X_test), y_test)  \n",
    "    models = res_stack.T[0]\n",
    "    score_train = res_stack.T[1]\n",
    "    score_test = res_stack.T[2]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.scatter(models, score_train, label='Train')\n",
    "    plt.scatter(models, score_test, label='Test')\n",
    "    plt.title('Model scores: r2')\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return res_stack\n",
    "\n",
    "def score_stacking(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute the score of the stacked estimator and of each level_0 estimator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: accuracy of estimators on train and test sets\n",
    "    res_stack: accuracy of estimators on train and test sets in a tabular\n",
    "    plotting: model importance according to performance\n",
    "    mod_imp: model importance in a tabular\n",
    "    \"\"\"     \n",
    "    if is_classifier(model):\n",
    "       res_stack = score_stacking_c(model, X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "       res_stack = score_stacking_r(model, X_train, y_train, X_test, y_test) \n",
    "    nb_estimators = len(model.estimators_)\n",
    "    res_level_0 = res_stack[0:nb_estimators]\n",
    "    mod_imp = np.delete(res_level_0[res_level_0[:, 2].argsort()], 1, axis=1)\n",
    "    mod_imp.T[1] = mod_imp.T[1] / np.sum(mod_imp.T[1])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(mod_imp.T[0], mod_imp.T[1])\n",
    "    ax.set_title(\"Model Importance according to performance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return res_stack, mod_imp\n",
    "\n",
    "def find_coeff(model):\n",
    "    \"\"\"\n",
    "    Searches the wrapped model for the feature importances parameter.\n",
    "    \"\"\"\n",
    "    for attr in (\"feature_importances_\", \"coef_\"):\n",
    "        try:\n",
    "           return getattr(model, attr)\n",
    "        except AttributeError:\n",
    "           continue\n",
    "\n",
    "        raise YellowbrickTypeError(\n",
    "           \"could not find feature importances param on {}\".format(\n",
    "                model.__class__.__name__\n",
    "           )\n",
    "        )\n",
    "        \n",
    "def model_importance_c(model, level_1_model):\n",
    "    \"\"\"\n",
    "    Compute the model importance depending on final estimator coefficients for classification\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mod_imp: sorted array of model importance \n",
    "    \"\"\"        \n",
    "    level_0 = np.array(list(model.named_estimators_.keys()))\n",
    "    n_classes = model.classes_.shape[0]\n",
    "    n_models = len(model.estimators_)\n",
    "    model_coeff = find_coeff(model.final_estimator_)\n",
    "    \n",
    "    if level_1_model == 'tree':\n",
    "       if len(model_coeff) == n_models:\n",
    "          coeff = model_coeff.reshape(n_models)  \n",
    "       else:\n",
    "          coeff = sum(model_coeff.reshape(n_classes,n_models))\n",
    "            \n",
    "    if level_1_model == 'regression':\n",
    "       if len(model_coeff[0]) == n_models:\n",
    "          coeff = model_coeff.reshape(n_models)  \n",
    "       else:\n",
    "          coeff = sum(model_coeff.reshape(n_classes,n_models,n_classes)[i].T[i] for i in range(n_classes))\n",
    "            \n",
    "    model_importance = np.empty((len(level_0), 2), dtype='object')\n",
    "    for ind in range(len(level_0)):\n",
    "        model_importance[ind, 0] = level_0[ind]\n",
    "        model_importance[ind, 1] = np.abs(coeff[ind])\n",
    "    return model_importance[model_importance[:, 1].argsort()]\n",
    "\n",
    "def model_importance_r(model, level_1_model):\n",
    "    \"\"\"\n",
    "    Compute the model importance depending on final estimator coefficients for regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mod_imp: sorted array of model importance \n",
    "    \"\"\"         \n",
    "    level_0 = np.array(list(model.named_estimators_.keys()))\n",
    "    coeff = find_coeff(model.final_estimator_)\n",
    "    model_importance = np.empty((len(level_0), 2), dtype='object')\n",
    "    for ind in range(len(level_0)):\n",
    "        model_importance[ind, 0] = level_0[ind]\n",
    "        model_importance[ind, 1] = np.abs(coeff[ind])\n",
    "    return model_importance[model_importance[:, 1].argsort()]\n",
    "\n",
    "def plot_model_importance(model, level_1_model):\n",
    "    \"\"\"\n",
    "    Compute the model importance depending on final estimator coefficients\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: model importance according to aggragator coefficients\n",
    "    mod_imp: sorted array of model importance \n",
    "    \"\"\"      \n",
    "    if is_classifier(model):\n",
    "       mod_imp = model_importance_c(model, level_1_model)\n",
    "    else:\n",
    "       mod_imp = model_importance_r(model, level_1_model)\n",
    "    mod_imp.T[1] = mod_imp.T[1] / np.sum(mod_imp.T[1])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(mod_imp.T[0], mod_imp.T[1])\n",
    "    ax.set_title(\"Model Importance according to aggragator coefficients\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return mod_imp\n",
    "\n",
    "def plot_perm_importance(model, X, y, CPU):\n",
    "    \"\"\"\n",
    "    Compute the feature permutation importance\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X: feature dataframe\n",
    "    y: target dataframe\n",
    "    CPU: boolean for CPU training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: feature permutation importance\n",
    "    perm_imp: sorted array of feature permutation importance\n",
    "    \"\"\"       \n",
    "    if is_classifier(model):\n",
    "       scoring = 'accuracy'\n",
    "    else:\n",
    "       scoring = 'r2'  \n",
    "    if CPU==True:\n",
    "       result = permutation_importance(model, X, y, scoring=scoring, n_repeats=10, n_jobs=-1)\n",
    "    else:\n",
    "       result = permutation_importance(model, X, y, scoring=scoring, n_repeats=10)\n",
    "    sorted_idx = result.importances_mean.argsort()\n",
    "    perm_imp = np.array([X.columns[sorted_idx], result.importances[sorted_idx].mean(axis=1).T]).T\n",
    "    perm_imp.T[1] = perm_imp.T[1] / np.sum(perm_imp.T[1])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(perm_imp.T[0], perm_imp.T[1])\n",
    "    ax.set_title(\"Permutation Importance\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return perm_imp\n",
    "\n",
    "def plot_partial_dependence_c(model, X, features, CPU):\n",
    "    \"\"\"\n",
    "    Plot partial dependence of features for a given classification estimator and a given dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X: feature dataframe\n",
    "    features: list of features\n",
    "    CPU: boolean for CPU training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: partial dependence of input features\n",
    "    \"\"\"      \n",
    "    target = model.classes_\n",
    "    for ind in range(len(target)):\n",
    "        fig, ax = plt.subplots(figsize=(16, 8))\n",
    "        if CPU==True:\n",
    "           display = PartialDependenceDisplay.from_estimator(\n",
    "                     estimator = model,\n",
    "                     X = X,\n",
    "                     features = features,\n",
    "                     target = target[ind],\n",
    "                     n_cols = 2,\n",
    "                     kind = \"both\",\n",
    "                     subsample=50,\n",
    "                     n_jobs = -1,\n",
    "                     grid_resolution = 20,\n",
    "                     ice_lines_kw = {\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
    "                     pd_line_kw = {\"color\": \"tab:orange\", \"linestyle\": \"--\"},\n",
    "                     ax = ax,\n",
    "                     )\n",
    "        else:\n",
    "           display = PartialDependenceDisplay.from_estimator(\n",
    "                     estimator = model,\n",
    "                     X = X,\n",
    "                     features = features,\n",
    "                     target = target[ind],\n",
    "                     n_cols = 2,\n",
    "                     kind = \"both\",\n",
    "                     subsample=50,\n",
    "                     grid_resolution = 20,\n",
    "                     ice_lines_kw = {\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
    "                     pd_line_kw = {\"color\": \"tab:orange\", \"linestyle\": \"--\"},\n",
    "                     ax = ax,\n",
    "                     )\n",
    "        display.figure_.suptitle(\"Partial dependence for class \" + str(target[ind]))\n",
    "        display.figure_.subplots_adjust(hspace=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "def plot_partial_dependence_r(model, X, features, CPU):\n",
    "    \"\"\"\n",
    "    Plot partial dependence of features for a given regression estimator and a given dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X: feature dataframe\n",
    "    features: list of features\n",
    "    CPU: boolean for CPU training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: partial dependence of input features\n",
    "    \"\"\"      \n",
    "    fig, ax = plt.subplots(figsize=(16, 8))\n",
    "    if CPU==True:\n",
    "       display = PartialDependenceDisplay.from_estimator(\n",
    "                 estimator = model,\n",
    "                 X = X,\n",
    "                 features = features,\n",
    "                 n_cols = 2,\n",
    "                 kind=\"both\",\n",
    "                 subsample=50,\n",
    "                 n_jobs=-1,\n",
    "                 grid_resolution=20,\n",
    "                 ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
    "                 pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"},\n",
    "                 ax = ax,\n",
    "                 )\n",
    "    else:\n",
    "       display = PartialDependenceDisplay.from_estimator(\n",
    "                 estimator = model,\n",
    "                 X = X,\n",
    "                 features = features,\n",
    "                 n_cols = 2,\n",
    "                 kind=\"both\",\n",
    "                 subsample=50,\n",
    "                 grid_resolution=20,\n",
    "                 ice_lines_kw={\"color\": \"tab:blue\", \"alpha\": 0.2, \"linewidth\": 0.5},\n",
    "                 pd_line_kw={\"color\": \"tab:orange\", \"linestyle\": \"--\"},\n",
    "                 ax = ax,\n",
    "                 )\n",
    "    display.figure_.suptitle(\"Partial dependence\")\n",
    "    display.figure_.subplots_adjust(hspace=0.3)\n",
    "    plt.show() \n",
    "\n",
    "def plot_partial_dependence(model, X, features, CPU):\n",
    "    \"\"\"\n",
    "    Plot partial dependence of features for a given estimator and a given dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X: feature dataframe\n",
    "    features: list of features, if features = [], partial dependences will be plot for all numeric features\n",
    "    CPU: boolean for CPU training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: partial dependence of input features\n",
    "    \"\"\"    \n",
    "    # if input list of features is empty, we use the list of numeric features\n",
    "    if features == []:\n",
    "       features = X.select_dtypes([np.number]).columns.tolist() \n",
    "    else:\n",
    "    #  we keep only numeric features    \n",
    "       features = np.intersect1d(features, X.select_dtypes([np.number]).columns.tolist()).tolist() \n",
    "        \n",
    "    if features == []:\n",
    "       return \"No numeric feature\"\n",
    "    else:\n",
    "       if is_classifier(model):\n",
    "          plot_partial_dependence_c(model, X, features, CPU)\n",
    "       else:\n",
    "          plot_partial_dependence_r(model, X, features, CPU)\n",
    "\n",
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Plot learning curves of Keras neural network\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    history: history of Keras neural network\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: learning curves of Keras neural network\n",
    "    \"\"\"     \n",
    "    pd.DataFrame(history.history).plot(figsize=(12, 9))\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def K_confusion_matrix(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix of a classification estimator on train and test sets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: confusion matrix on train and test sets\n",
    "    \"\"\"     \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(X_train)\n",
    "    if len(y_pred.shape)>1:\n",
    "       y_pred = np.around(y_pred).astype(int)\n",
    "       y_pred = np.argmax(y_pred, axis=1)\n",
    "       y_train = y_train.idxmax(axis=1)\n",
    "    cm = confusion_matrix(y_train, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion matrix on train set')\n",
    "    plt.show()\n",
    "    y_pred = model.predict(X_test)\n",
    "    if len(y_pred.shape)>1:\n",
    "       y_pred = np.around(y_pred).astype(int)\n",
    "       y_pred = np.argmax(y_pred, axis=1)\n",
    "       y_test = y_test.idxmax(axis=1)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title('Confusion matrix on test set')\n",
    "    plt.show()\n",
    "    \n",
    "def K_classification_report(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plot classification report of a classification estimator on train and test sets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plotting: classification report on train and test sets\n",
    "    \"\"\"        \n",
    "    y_pred = model.predict(X_train)\n",
    "    if len(y_pred.shape)>1:\n",
    "       y_pred = np.around(y_pred).astype(int)\n",
    "       y_pred = np.argmax(y_pred, axis=1)\n",
    "       y_train = y_train.idxmax(axis=1)\n",
    "    cr=classification_report(y_train, y_pred, output_dict=True)\n",
    "    display(pd.DataFrame(cr).transpose().style.set_caption(\"Classification report on train set\"))\n",
    "    y_pred = model.predict(X_test)\n",
    "    if len(y_pred.shape)>1:\n",
    "       y_pred = np.around(y_pred).astype(int)\n",
    "       y_pred = np.argmax(y_pred, axis=1)\n",
    "       y_test = y_test.idxmax(axis=1)\n",
    "    cr=classification_report(y_test, y_pred, output_dict=True)\n",
    "    display(pd.DataFrame(cr).transpose().style.set_caption(\"Classification report on test set\"))\n",
    "    \n",
    "def K_r2(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute R^2 of a regression estimator on train and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    X_train: train feature dataframe \n",
    "    X_test: test feature dataframe\n",
    "    y_train: train target dataframe\n",
    "    y_test: test target dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array: scores on train and test sets\n",
    "    \"\"\"         \n",
    "    y_pred_train = model.predict(X_train)    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    dr2={'train': [r2_score(y_train, y_pred_train)],\\\n",
    "         'test': [r2_score(y_test, y_pred_test)]}\n",
    "    display(pd.DataFrame(data=dr2).style.hide_index())\n",
    "     \n",
    "def fastapi_server(model, model_name, X, y):\n",
    "    \"\"\"\n",
    "    Generate the fastAPI server file, and save it in the current folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: estimator obtained after fitting\n",
    "    model_name : name of the saved model\n",
    "    X: feature dataframe \n",
    "    y: target dataframe\n",
    "    \n",
    "    \"\"\"   \n",
    "    string = \"\"\n",
    "    string = string  + \"from fastapi import FastAPI\\n\"\n",
    "    string = string  + \"from loguru import logger\\n\"\n",
    "    string = string  + \"from joblib import load\\n\"\n",
    "    string = string  + \"import pandas as pd\\n\"\n",
    "    string = string  + \"import numpy as np\\n\"\n",
    "    string = string  + \"import uvicorn\\n\"\n",
    "    string = string  + \"import ast\\n\"\n",
    "    string = string  + \"import time\\n\"\n",
    "    string = string  + \"from sklearn.base import is_classifier\\n\"\n",
    "    string = string  + \"from pydantic import BaseModel\\n\"\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"# Creating FastAPI instance\\n\"\n",
    "    string = string  + \"app = FastAPI()\\n\"\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"# Creating class to define the request body\\n\"\n",
    "    string = string  + \"# and the type hints of each attribute\\n\"\n",
    "\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"class request_body(BaseModel):\\n\"\n",
    "    for ind in range(X.dtypes.shape[0]):\n",
    "        if str(X.dtypes[ind])[0:5]=='float':\n",
    "           string = string + '      ' + X.columns[ind] + ': float\\n'\n",
    "        if str(X.dtypes[ind])[0:3]=='int':\n",
    "           string = string + '      ' + X.columns[ind] + ': int\\n'\n",
    "        if str(X.dtypes[ind])[0:4]=='uint':\n",
    "           string = string + '      ' + X.columns[ind] + ': int\\n'\n",
    "        if str(X.dtypes[ind])[0:6]=='object':\n",
    "           string = string + '      ' + X.columns[ind] + ': str\\n'\n",
    "        if str(X.dtypes[ind])[0:4]=='bool':\n",
    "           string = string + '      ' + X.columns[ind] + ': bool\\n'\n",
    "\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"# read dataframe schema\\n\"\n",
    "    string = string  + \"schema = pd.read_csv('schema.csv')\" \n",
    "    string = string  + \"\\n\"        \n",
    "    modulename = 'keras'\n",
    "    keras_bool = False\n",
    "    if modulename in sys.modules:\n",
    "       keras_bool = True \n",
    "       from EZS_deps.EZS_tech_func import keras_nn\n",
    "       if is_classifier(model):\n",
    "          string = string  + keras_nn('classification')\n",
    "       else:\n",
    "          string = string  + keras_nn('regression')\n",
    "        \n",
    "    string = string  + \"\\n\"        \n",
    "    string = string  + \"model = load('\" + model_name + \"')\\n\"\n",
    "\n",
    "    string = string  + \"\\n\"\n",
    "    if is_classifier(model):\n",
    "       string = string  + \"classes = \" + str(y.unique().tolist()) + \"\\n\"\n",
    "    \n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"@app.get('/ping')\\n\"\n",
    "    string = string  + \"def pong():\\n\"\n",
    "    string = string  + \"    return {'ping': 'pong!'}\\n\"\n",
    "    \n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"@app.post('/predict')\\n\"\n",
    "    string = string  + \"def predict(data : request_body):\\n\"\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"    elaps_start_time = time.time()\\n\"\n",
    "    string = string  + \"    cpu_start_time = time.process_time()\\n\"\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"    # Making the data in a form suitable for prediction\\n\"\n",
    "    string = string  + \"    test_data = [[\\n\"\n",
    "    for ind in range(X.columns.shape[0]):\n",
    "        string = string  + \"              data.\" + X.columns[ind] + ',\\n'\n",
    "    string = string  + \"    ]]\\n\"\n",
    "    \n",
    "    string = string  + \"\\n\"\n",
    "    \n",
    "    string = string  + \"    # Check input data\\n\"\n",
    "    string = string  + \"    data_err = []\\n\"\n",
    "    string = string  + \"    for ind in range(len(test_data[0])):\\n\"\n",
    "    string = string  + \"        if schema.iloc[ind][1] == 'num':\\n\"\n",
    "    string = string  + \"           interval = ast.literal_eval(schema.iloc[ind][2])\\n\"\n",
    "    string = string  + \"           if (test_data[0][ind] < interval[0]) | (test_data[0][ind] > interval[1]):\\n\"\n",
    "    string = string  + \"              data_err.append(schema.iloc[ind][0])\\n\"\n",
    "    string = string  + \"        if schema.iloc[ind][1] == 'cat':\\n\"\n",
    "    string = string  + \"           domain = ast.literal_eval(schema.iloc[ind][2])\\n\"\n",
    "    string = string  + \"           if not(np.isin(test_data[0][ind], domain)):\\n\"\n",
    "    string = string  + \"              data_err.append(schema.iloc[ind][0])\\n\"\n",
    "    string = string  + \"\\n\"\n",
    "\n",
    "                \n",
    "    if is_classifier(model):\n",
    "       string = string  + \"    # Predicting the Class\\n\"\n",
    "       string = string  + \"    result = model.predict(pd.DataFrame(test_data,\\n\"\n",
    "       string = string  + \"                                        columns=[\\n\"\n",
    "       for ind in range(X.columns.shape[0]):\n",
    "           string = string  + \"                                                  '\" + X.columns[ind] + \"',\\n\"\n",
    "       string = string  + \"                          ]))[0].item()\\n\"\n",
    "       string = string  + \"\\n\"\n",
    "\n",
    "       string = string  + \"    elaps_end_time = time.time()\\n\"\n",
    "       string = string  + \"    cpu_end_time = time.process_time()\\n\"\n",
    "       string = string  + \"    elapsed_time = np.round((elaps_end_time - elaps_start_time) * 1000)\\n\"\n",
    "       string = string  + \"    elaps = str(elapsed_time) + 'ms'\\n\"\n",
    "       string = string  + \"    cpu_time = np.round((cpu_end_time - cpu_start_time) * 1000)\\n\"\n",
    "       string = string  + \"    cpu = str(cpu_time) + 'ms'\\n\"       \n",
    "       string = string  + \"\\n\"\n",
    "       string = string  + \"    # Return the Result\\n\"\n",
    "       string = string  + \"    return { 'class' : classes[result], 'error' : data_err, 'elapsed time' : elaps, 'cpu time' : cpu}\\n\"\n",
    "    else: \n",
    "       string = string  + \"    # Predicting the regression value\\n\"\n",
    "       if keras_bool: \n",
    "          string = string  + \"    result = model.predict(pd.DataFrame(np.array([test_data[0],]*2),\\n\"\n",
    "       else:\n",
    "          string = string  + \"    result = model.predict(pd.DataFrame(test_data,\\n\"        \n",
    "       string = string  + \"                                        columns=[\\n\"\n",
    "       for ind in range(X.columns.shape[0]):\n",
    "           string = string  + \"                                                 '\" + X.columns[ind] + \"',\\n\"\n",
    "       string = string  + \"                          ]))[0].item()\\n\"\n",
    "       string = string  + \"\\n\"\n",
    "       string = string  + \"    elaps_end_time = time.time()\\n\"\n",
    "       string = string  + \"    cpu_end_time = time.process_time()\\n\"\n",
    "       string = string  + \"    elapsed_time = np.round((elaps_end_time - elaps_start_time) * 1000)\\n\"\n",
    "       string = string  + \"    elaps = str(elapsed_time) + 'ms'\\n\"\n",
    "       string = string  + \"    cpu_time = np.round((cpu_end_time - cpu_start_time) * 1000)\\n\"\n",
    "       string = string  + \"    cpu = str(cpu_time) + 'ms'\\n\"\n",
    "       string = string  + \"\\n\"\n",
    "       string = string  + \"    # Return the Result\\n\"\n",
    "       string = string  + \"    return { 'regression_value' : result, 'error' : data_err, 'elapsed time' : elaps, 'cpu time' : cpu}\\n\"\n",
    "    \n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"from pyngrok import ngrok\\n\"\n",
    "    string = string  + \"ngrok_tunnel = ngrok.connect(8000)\\n\"\n",
    "    string = string  + \"ngrok_tunnel\\n\"\n",
    "\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"import nest_asyncio\\n\"\n",
    "    string = string  + \"\\n\"\n",
    "    string = string  + \"nest_asyncio.apply()\\n\"\n",
    "    string = string  + \"uvicorn.run(app, port=8000)\\n\"\n",
    "\n",
    "    file_server = open(\"server.py\", \"w\") \n",
    "    file_server.write(string)\n",
    "    file_server.close()  \n",
    "\n",
    "def store_data(name, level_1_model, score_stack_0, score_stack_1, score_stack_2, \n",
    "           model_imp_0, model_imp_1, model_imp_2, \n",
    "           feature_importance_0, feature_importance_1, feature_importance_2):\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect('/home/philippe/development/python/EZStacking/EZS_deps/EZS_store.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    search_problem = cursor.execute(\"SELECT name FROM problem WHERE name = ?\", (name,))\n",
    "    problem_name = search_problem.fetchone()\n",
    "    if problem_name == None:\n",
    "       cursor.execute(\"INSERT INTO problem (name, path , type, target) VALUES(?, ?, ?, ?)\", (name, path, problem_type, target_col))\n",
    "\n",
    "    search_version = cursor.execute(\"SELECT MAX(version) FROM solution WHERE name = ?\", (name,))\n",
    "    row = search_version.fetchone()\n",
    "    if row == (None,):\n",
    "       version = 1\n",
    "    else:\n",
    "       version = row[0] + 1\n",
    "\n",
    "    cursor.execute(\"INSERT INTO solution (name, version, correlation, nb_model, nb_feature, score, test_size) VALUES(?, ?, ?, ?, ?, ?, ?)\", \\\n",
    "                    (name, version, threshold_corr, threshold_model, threshold_feature, threshold_score, test_size));\n",
    "\n",
    "    schema = pd.read_csv('schema.csv')\n",
    "    for ind in range(len(user_drop_cols)):\n",
    "        cursor.execute(\"INSERT INTO eda (name, version, feature, type, range, drop_user, drop_correlation, target)  VALUES(?, ?, ?, ?, ?, ?, ?, ?)\", \\\n",
    "                        (name, version, user_drop_cols[ind], None, None, 1, 0, 0));\n",
    "    for ind in range(schema.shape[0]):\n",
    "        if schema['column_name'][ind] in correlated_features:\n",
    "           drop_correlation = True\n",
    "        else:\n",
    "           drop_correlation = False\n",
    "\n",
    "        cursor.execute(\"INSERT INTO eda (name, version, feature, type, range, drop_user, drop_correlation, target)  VALUES(?, ?, ?, ?, ?, ?, ?, ?)\", \\\n",
    "                        (name, version, schema['column_name'][ind], schema['column_type'][ind], schema['column_range'][ind], 0, drop_correlation, 0));\n",
    "\n",
    "    cursor.execute(\"INSERT INTO eda (name, version, feature, type, range, drop_user, drop_correlation, target)  VALUES(?, ?, ?, ?, ?, ?, ?, ?)\", \\\n",
    "                        (name, version, target_col, None, None, 0, 0, 1));\n",
    "\n",
    "    for ind in range(3):\n",
    "        cursor.execute(\"INSERT INTO model (name, version, step, L1_model) VALUES (?, ?, ?, ?)\", \\\n",
    "                        (name, version, ind+1, level_1_model));\n",
    "\n",
    "        score_stack = locals()[\"_\".join(['score_stack', str(ind)])]\n",
    "        for ind2 in range(score_stack.shape[0]):\n",
    "            cursor.execute(\"INSERT INTO model_score (name, version, step, model, train_score, test_score) VALUES(?, ?, ?, ?, ?, ?)\", \\\n",
    "                            (name, version, ind+1, score_stack[ind2,0], score_stack[ind2,1], score_stack[ind2,2]));\n",
    "\n",
    "        model_imp = locals()[\"_\".join(['model_imp', str(ind)])]\n",
    "        for ind2 in range(model_imp.shape[0]):\n",
    "            cursor.execute(\"INSERT INTO model_importance (name, version, step, model, importance) VALUES(?, ?, ?, ?, ?)\", \\\n",
    "                            (name, version, ind+1, model_imp[ind2,0], model_imp[ind2,1]));\n",
    "\n",
    "        feature_importance = locals()[\"_\".join(['feature_importance', str(ind)])]\n",
    "        for ind2 in range(feature_importance.shape[0]):\n",
    "            cursor.execute(\"INSERT INTO feature_importance (name, version, step, feature, importance) VALUES(?, ?, ?, ?, ?)\", \\\n",
    "                            (name, version, ind+1, feature_importance[ind2,0], feature_importance[ind2,1]));\n",
    "\n",
    "    # cursor.execute(\"DELETE FROM problem WHERE name = ?\", (name,))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from math import sin, trunc\n",
    "\n",
    "# Constants for the provided location\n",
    "longitude = 121.56387313547269\n",
    "\n",
    "# Function to calculate days since Jan 1, 2000\n",
    "def epoch_2k_day(query_date):\n",
    "    epoch_year = datetime(2000, 1, 1, 12, 0, 0)\n",
    "    return (query_date - epoch_year).days + (query_date - epoch_year).seconds / 86400.0\n",
    "\n",
    "# Equation of Time (EOT) function\n",
    "def equation_of_time(days):\n",
    "    cycle = round(days / 365.25)\n",
    "    theta = 0.0172024 * (days - 365.25 * cycle)\n",
    "    amp1 = 7.36303 - cycle * 0.00009\n",
    "    amp2 = 9.92465 - cycle * 0.00014\n",
    "    rho1 = 3.07892 - cycle * 0.00019\n",
    "    rho2 = -1.38995 + cycle * 0.00013\n",
    "    eot1 = amp1 * sin(1 * (theta + rho1))\n",
    "    eot2 = amp2 * sin(2 * (theta + rho2))\n",
    "    eot3 = 0.31730 * sin(3 * (theta - 0.94686))\n",
    "    eot4 = 0.21922 * sin(4 * (theta - 0.60716))\n",
    "    return 0.00526 + eot1 + eot2 + eot3 + eot4  # minutes\n",
    "\n",
    "# Function to calculate solar noon with precision up to seconds\n",
    "def solar_noon_precise(longitude_offset_minutes, timezone_offset=8):\n",
    "    noon_minutes = 12 * 60\n",
    "    total_minutes = noon_minutes + longitude_offset_minutes + (timezone_offset * 60)\n",
    "    hours = trunc(total_minutes / 60)\n",
    "    minutes = trunc(total_minutes - (hours * 60))\n",
    "    seconds = round((total_minutes - (hours * 60) - minutes) * 60)\n",
    "    return f'{int(hours):02}:{int(minutes):02}:{int(seconds):02}'\n",
    "\n",
    "# Function to calculate solar noon offset\n",
    "def longitude_offset(eot, longitude):\n",
    "    return -1 * (eot + (4 * longitude))\n",
    "\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    Perform feature engineering on the dataset, including time-based features,\n",
    "    rolling statistics, and interaction features.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): Input dataset containing weather and power data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with newly created features.\n",
    "    \"\"\"\n",
    "    # Convert DateTime to datetime format\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['month'] = df['DateTime'].dt.month\n",
    "    df['day'] = df['DateTime'].dt.day\n",
    "    df['hour'] = df['DateTime'].dt.hour\n",
    "    df['minute'] = df['DateTime'].dt.minute\n",
    "    \n",
    "    # Create cyclic features (sin and cos) for time of day\n",
    "    df['day_seconds'] = df['hour'] * 3600 + df['minute'] * 60 + df['DateTime'].dt.second\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_seconds'] / (24 * 3600))\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_seconds'] / (24 * 3600))\n",
    "    #df['min_sin'] = np.sin(2 * np.pi * df['day_seconds'] / (24 * 60))\n",
    "    #df['min_cos'] = np.cos(2 * np.pi * df['day_seconds'] / (24 * 60))\n",
    "\n",
    "\n",
    "    df['day_in_week'] = df['DateTime'].dt.dayofweek\n",
    "\n",
    "    # 新增 is_weekend 欄位\n",
    "    df['is_weekend'] = df['day_in_week'].isin([5, 6])\n",
    "    # Interaction features\n",
    "    #df['sunlight_temp_interaction'] = df['Sunlight(Lux)'] * df['Temperature(°C)']\n",
    "    \n",
    "\n",
    "    # Handle missing values by filling with mean\n",
    "    #df.fillna(df.mean(), inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411114c9-384a-47b9-a168-9e04041a923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('L1_Generated.csv')\n",
    "df = df.dropna(subset=['Power(mW)_x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21c2f6-8804-4aee-9e13-8f026188f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'Power(mW)_x'\n",
    "threshold_NaN = 0.5\n",
    "threshold_cat = 5\n",
    "threshold_Z = 3.0\n",
    "test_size = 0.33\n",
    "threshold_entropy = 0.75\n",
    "undersampling = False\n",
    "undersampler = 'Random'\n",
    "threshold_corr = 0.75\n",
    "threshold_model = 6\n",
    "threshold_score = 0.01\n",
    "threshold_feature = 50\n",
    "CPU = False\n",
    "level_1_model = 'regression'    \n",
    "#user_drop_cols = ['EWMA_Sunlight6', 'EWMA_Sunlight3','EWMA_Sunlight4' , 'Rolling_Mean_Sunlight', 'Power(mW)_y']\n",
    "user_drop_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc852678-4e0e-4009-95ab-8f8e90395f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(user_drop_cols, axis=1)\n",
    "df_copy = df.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11c3d0-d4af-425b-b390-8e2e781c0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataframe_structure(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a7d70-bbaf-4781-814c-fccad1d389d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c9f59-9a8a-4e29-803d-a34dc51612b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528bcf2-7fe7-4820-8e03-22c006020b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, drop_cols = drop_na(df, threshold_NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba13d00-451a-4d85-9fcf-7199d8451754",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_cols = np.unique(np.concatenate((drop_cols, user_drop_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15115d31-59db-4119-9e4e-1c68f8e59ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dropped_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52b720-b8c6-4580-9fbd-c0d7e0b69e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, encoded_cols = encoding(df, threshold_cat, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf183a-f6ca-43ff-ae11-29151579f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = imputation(df)\n",
    "df = downcast_dtypes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91bf6b-6728-4ded-bc60-1357dd3c5b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataframe_structure(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc737ea-13e7-43f2-8ff1-0e012935b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = outliers(df, threshold_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf7867-9d45-4495-9684-dcad28137a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank1d(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347610a-4489-4a4d-8c4d-0f750c9a3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank2d(df, algorithm='pearson');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005681e-3014-493d-92c1-7a3456d06c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank2d(df, algorithm='covariance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06c679-af1f-408b-964e-1e624172d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = correlated_columns(df, threshold_corr, target_col) \n",
    "dropped_cols = np.unique(np.concatenate((drop_cols, correlated_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c75adb-3303-4e87-87be-8acf970d3625",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14345b7a-ece8-4f6b-b7da-8f307955c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlation(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6fcaf-43b7-4d5c-8cd6-45d5503572cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correlation(X, y, method='mutual_info-regression');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232bd02f-e4ca-42ad-8301-12f89d72c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances(DecisionTreeRegressor(), X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0d15a-7336-408c-988f-69468e72de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances(ElasticNet(alpha=0.01, l1_ratio=0.5), X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d85ed-2861-436e-b4a1-a0db4fdfe90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_copy\n",
    "y = df[target_col]\n",
    "X = df.drop(target_col, axis=1)\n",
    "nb_features = len(X.columns.tolist())\n",
    "nb_targets = 1\n",
    "layer_size = nb_features + nb_targets + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbcc37-d70e-4e4b-852a-74949b882ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf43e4f-12e1-4344-a5ee-4ff02d0fc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(X, y, test_size=test_size, threshold_entropy=threshold_entropy, undersampling= undersampling, undersampler= undersampler, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f8ce4-5c8f-4bbf-a016-35f46dc2099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_selector = make_column_selector(dtype_include=object)\n",
    "num_selector = make_column_selector(dtype_include=np.number)\n",
    "cat_tree_processor = make_pipeline(SimpleImputer(strategy='most_frequent'), OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "num_tree_processor = make_pipeline(IterativeImputer(random_state=0, add_indicator=True))\n",
    "tree_preprocessor = make_pipeline(make_column_transformer((num_tree_processor, num_selector), (cat_tree_processor, cat_selector)), Decorrelator(threshold_corr))\n",
    "cat_ntree_processor = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "num_ntree_processor = make_pipeline(IterativeImputer(random_state=0, add_indicator=True), StandardScaler())\n",
    "ntree_preprocessor = make_pipeline(make_column_transformer((num_ntree_processor, num_selector), (cat_ntree_processor, cat_selector)), Decorrelator(threshold_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530b640-7f18-4a74-8c74-e0dd29464b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "level_0 = [ \n",
    "          #('GPRL', make_pipeline(ntree_preprocessor, GaussianProcessRegressor(kernel = ConstantKernel() * DotProduct() + ConstantKernel() + WhiteKernel(), random_state = random_state))), \n",
    "          #('GPRR', make_pipeline(ntree_preprocessor, GaussianProcessRegressor(kernel = ConstantKernel() * RBF() + ConstantKernel() + WhiteKernel(), random_state = random_state))), \n",
    "          #('GPRQ', make_pipeline(ntree_preprocessor, GaussianProcessRegressor(kernel = ConstantKernel() * RationalQuadratic() + ConstantKernel() + WhiteKernel(), random_state = random_state))), \n",
    "          #('DTRF', make_pipeline(tree_preprocessor, DecisionTreeRegressor(criterion='friedman_mse', random_state = random_state))), \n",
    "          #('DTRA', make_pipeline(tree_preprocessor, DecisionTreeRegressor(criterion='absolute_error', random_state = random_state))), \n",
    "          #('DTRP', make_pipeline(tree_preprocessor, DecisionTreeRegressor(criterion='poisson', random_state = random_state))), \n",
    "          #('RFRS', make_pipeline(tree_preprocessor, RandomForestRegressor(criterion='squared_error', n_estimators=100, random_state = random_state))), \n",
    "          #('RFRA', make_pipeline(tree_preprocessor, RandomForestRegressor(criterion='absolute_error', n_estimators=100, random_state = random_state))), \n",
    "          #('RFRP', make_pipeline(tree_preprocessor, RandomForestRegressor(criterion='poisson', n_estimators=100, random_state = random_state))), \n",
    "          #('ABR', make_pipeline(tree_preprocessor, AdaBoostRegressor(random_state = random_state))), \n",
    "          ('HGBR', make_pipeline(tree_preprocessor, HistGradientBoostingRegressor(loss = 'absolute_error', early_stopping=True, random_state = random_state))), \n",
    "          #('ELNE', make_pipeline(ntree_preprocessor, ElasticNet(alpha=0.01, l1_ratio=0.15, random_state = random_state))), \n",
    "          #('ELNECV', make_pipeline(ntree_preprocessor, ElasticNetCV(cv=5, random_state = random_state))), \n",
    "          #('LINR', make_pipeline(ntree_preprocessor, LinearRegression())), \n",
    "          #('MLPR1', make_pipeline(ntree_preprocessor, MLPRegressor(hidden_layer_sizes = (layer_size, ), max_iter=2000, early_stopping=True, random_state = random_state))), \n",
    "          #('MLPR2', make_pipeline(ntree_preprocessor, MLPRegressor(hidden_layer_sizes = (layer_size, layer_size,), max_iter=2000, early_stopping=True, random_state = random_state))), \n",
    "          #('KNRU', make_pipeline(ntree_preprocessor, KNeighborsRegressor(weights='uniform'))), \n",
    "          #('KNRD', make_pipeline(ntree_preprocessor, KNeighborsRegressor(weights='distance'))), \n",
    "          #('SVR', make_pipeline(ntree_preprocessor, SVR())),  # Support Vector Regression\n",
    "          #('XGBR', make_pipeline(tree_preprocessor, xgb)),  # XGBoost Regressor\n",
    "          #('LGBMR', make_pipeline(tree_preprocessor, lgb.LGBMRegressor(objective='regression_l1', random_state=random_state))),\n",
    "          #('CatBoostR', make_pipeline(tree_preprocessor, CatBoostRegressor(verbose=0, random_state=random_state))),  # CatBoost Regressor\n",
    "          #('BayesianRidge', make_pipeline(ntree_preprocessor, BayesianRidge())),  # Bayesian Ridge Regression\n",
    "          #('Lasso', make_pipeline(ntree_preprocessor, Lasso(alpha=0.01, random_state=random_state))),  # Lasso Regression\n",
    "           ('LGBM', make_pipeline(tree_preprocessor, LGBMRegressor(objective='regression', n_estimators=15000, learning_rate=0.15, random_state=random_state, n_jobs=-1, force_row_wise=True ))),\n",
    "           ('LGBMx', make_pipeline(tree_preprocessor, LGBMRegressor(objective='mae',n_estimators= 15000,learning_rate=0.15,random_state=random_state,n_jobs=-1,force_row_wise=True)))\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c857967-d0fc-455b-9845-f219e841dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_1 =  LGBMRegressor(objective='mae', n_estimators=10000, learning_rate=0.1, random_state=random_state, n_jobs = -1, force_row_wise=True,extra_trees=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c45042-5e6c-438c-9e26-a16f27066ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingRegressor(level_0, final_estimator=level_1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a036c0d-736c-4413-a7c5-ae43d575e657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "set_config(display='diagram') \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bf6036-7a36-4c10-b8f4-8313aca70c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_stack_0, mod_imp_score_0 = score_stacking(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6833c4-2967-4caf-b1fa-fe39592e4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_error(model, X_train, y_train, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bbf9be-630a-48f4-883b-bec4288d8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d57fef-c5dd-45f0-a764-1b283f83189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = pd.read_csv('upload_24.csv')\n",
    "#upload2 = pd.read_csv('upload_unfit_two_LocationCode.csv')\n",
    "upload.rename(columns={\"Power(mW)\": \"Power(mW)_y\"}, inplace=True)\n",
    "#upload2.rename(columns={\"Power(mW)\": \"Power(mW)_y\"}, inplace=True)\n",
    "\n",
    "#upload = pd.concat([upload1, upload2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1850cd4-579f-4da0-b772-c1fc79a758aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = feature_engineering(upload)\n",
    "longitude = 121.56387313547269\n",
    "upload['Date'] = pd.to_datetime(upload['DateTime']).dt.date\n",
    "\n",
    "# Create a dictionary to store solar noon for each unique day\n",
    "solar_noon_dict_precise = {}\n",
    "\n",
    "for date in upload['Date'].unique():\n",
    "    # Calculate days since 2000 for this date\n",
    "    query_date = datetime.combine(date, datetime.min.time())\n",
    "    days_since_2000 = epoch_2k_day(query_date)\n",
    "    eot = equation_of_time(days_since_2000)\n",
    "    offset = longitude_offset(eot, longitude)\n",
    "    solar_noon_time = solar_noon_precise(offset, timezone_offset=8)  # Adjust for UTC+8\n",
    "    solar_noon_dict_precise[date] = solar_noon_time\n",
    "\n",
    "# Map the solar noon time back to the DataFrame\n",
    "upload['SolarNoon'] = upload['Date'].map(solar_noon_dict_precise)\n",
    "\n",
    "# Convert SolarNoon to datetime object for comparison\n",
    "upload['SolarNoon'] = pd.to_datetime(upload['Date'].astype(str) + ' ' + upload['SolarNoon'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Convert DateTime to actual datetime format\n",
    "upload['DateTime'] = pd.to_datetime(upload['DateTime'])\n",
    "\n",
    "# Calculate the time difference between DateTime and SolarNoon\n",
    "upload['TimeDifference'] = (upload['DateTime'] - upload['SolarNoon']).abs()\n",
    "upload['TimeDifference_hours'] = upload['TimeDifference'].dt.total_seconds() / 3600\n",
    "# Display the updated DataFrame with time difference\n",
    "upload[['DateTime', 'SolarNoon', 'TimeDifference_hours']].head()\n",
    "upload.drop(columns=['TimeDifference', 'SolarNoon','Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e369710-681e-4cc6-8fdc-d4e70b270f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "upload['答案'] = model.predict(upload)\n",
    "upload['答案'] = upload['答案'].apply(lambda x: max(x, 0))\n",
    "\n",
    "submit = upload.groupby('序號', as_index=False)['答案'].mean()\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc6c8d-8e6a-4b39-ac74-d3c6ac7698c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit_24_0412_4.csv' , index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
